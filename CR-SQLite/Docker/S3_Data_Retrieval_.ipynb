{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a7d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aws-public-blockchain/v1.0/eth/blocks/date=2015-07-30', 'aws-public-blockchain/v1.0/eth/blocks/date=2015-07-31', 'aws-public-blockchain/v1.0/eth/blocks/date=2015-08-01', 'aws-public-blockchain/v1.0/eth/blocks/date=2015-08-02', 'aws-public-blockchain/v1.0/eth/blocks/date=2015-08-03']\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "\n",
    "directory_path = 'aws-public-blockchain/v1.0/eth/blocks'\n",
    "\n",
    "\n",
    "files = fs.ls(directory_path)\n",
    "\n",
    "\n",
    "parquet_files = [file for file in files]\n",
    "\n",
    "print(parquet_files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf76ec76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m         dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Concatenate all the DataFrames in the list into a single DataFrame\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# This combines the data from the top 5 .parquet files into one dataset\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Display the head of the combined DataFrame to verify its contents\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "# Setup access to the S3 bucket with s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Specify the directory path in the S3 bucket where the .parquet files are located\n",
    "directory_path = 'aws-public-blockchain/v1.0/eth/blocks'\n",
    "\n",
    "# List all files in the specified directory of the S3 bucket\n",
    "#directory_path = 's3://aws-public-blockchain/v1.0/eth/token_transfers'\n",
    "\n",
    "# Ensure you're getting the correct full paths for the .parquet files\n",
    "files = fs.ls(directory_path)\n",
    "parquet_files = [f's3://{file}' for file in files if file.endswith('.parquet')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames loaded from each .parquet file\n",
    "dfs = []\n",
    "\n",
    "# Loop through the top 5 .parquet file paths\n",
    "for file_path in parquet_files[:5]:\n",
    "    # Use s3fs to open and read each .parquet file into a pandas DataFrame\n",
    "    with fs.open(file_path, mode='rb') as f:\n",
    "        df = pd.read_parquet(f, engine='pyarrow')\n",
    "        # Append the loaded DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "# This combines the data from the top 5 .parquet files into one dataset\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the head of the combined DataFrame to verify its contents\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509cad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['difficulty', 'hash', 'miner', 'nonce', 'number', 'size', 'timestamp', 'total_difficulty', 'base_fee_per_gas', 'gas_limit', 'gas_used', 'extra_data', 'logs_bloom', 'parent_hash', 'state_root', 'receipts_root', 'transactions_root', 'sha3_uncles', 'transaction_count', 'date', 'last_modified']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "\n",
    "directory_path = 'aws-public-blockchain/v1.0/eth/blocks/date=2020-01-01'\n",
    "\n",
    "\n",
    "files = fs.ls(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "parquet_files = [file for file in files if file.endswith('.parquet')]\n",
    "\n",
    "\n",
    "file_path = 's3://' + parquet_files[0]\n",
    "\n",
    "with fs.open(file_path, mode='rb') as f:\n",
    "    df = pd.read_parquet(f, engine='pyarrow')\n",
    "\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7819f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      address  \\\n",
      "0  0xed5868f9ec0651f5a9797fad4716c1269b776730   \n",
      "1  0x5fe5b7546d1628f7348b023a0393de1fc825a4fd   \n",
      "2  0x02aeac9ca07bc56f37d5ec61b28a3dffd7cff0a3   \n",
      "3  0x90ab39213f1d947e2a3b8936ac37dca2556a9ece   \n",
      "4  0xa18d7182d5935578958a07fd7e9d062ddd418761   \n",
      "\n",
      "                                            bytecode  block_number  \\\n",
      "0  0x60606040523615610053576000357c01000000000000...         65407   \n",
      "1  0x606060405236156100615760e060020a60003504631c...         66126   \n",
      "2  0x606060405236156100775760e060020a600035046321...         66156   \n",
      "3  0x60606040526000357c01000000000000000000000000...         65327   \n",
      "4  0x60606040526000357c01000000000000000000000000...         66173   \n",
      "\n",
      "                                          block_hash     block_timestamp  \\\n",
      "0  0x0b14125ff9fdb36bef8a566c1a3cf6aa1356a366b2f4... 2015-08-10 19:53:14   \n",
      "1  0x80cb4a1f7d35cad112c1f035a5f31ccf52b450227fa6... 2015-08-10 23:26:20   \n",
      "2  0x4823b4a52e889d9bb4860fba9ee41b69907dbe1d703f... 2015-08-10 23:35:21   \n",
      "3  0x676494743d5df37250b8db3e23f052965f03f5c1af59... 2015-08-10 19:29:48   \n",
      "4  0xc2788ab09a3b0464019df2abc09e5ba9e869c53ef576... 2015-08-10 23:41:31   \n",
      "\n",
      "         date              last_modified  \n",
      "0  2015-08-10 2022-09-11 23:51:39.916963  \n",
      "1  2015-08-10 2022-09-11 23:51:49.252462  \n",
      "2  2015-08-10 2022-09-11 23:51:49.634849  \n",
      "3  2015-08-10 2022-09-11 23:51:38.899058  \n",
      "4  2015-08-10 2022-09-11 23:51:49.854120  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "\n",
    "directory_path = 's3://aws-public-blockchain/v1.0/eth/contracts/date=2015-08-10'\n",
    "\n",
    "\n",
    "files = fs.ls(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "parquet_files = [file for file in files if file.endswith('.parquet')]\n",
    "\n",
    "\n",
    "file_path = 's3://' + parquet_files[0]\n",
    "\n",
    "with fs.open(file_path, mode='rb') as f:\n",
    "    df = pd.read_parquet(f, engine='pyarrow')\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c149e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aws-public-blockchain/v1.0/eth/contracts/date=2015-08-10/part-00000-3759dbc2-5841-4cd8-b13c-e9776aa78129-c000.snappy.parquet']\n",
      "                                      address  \\\n",
      "0  0xed5868f9ec0651f5a9797fad4716c1269b776730   \n",
      "1  0x5fe5b7546d1628f7348b023a0393de1fc825a4fd   \n",
      "2  0x02aeac9ca07bc56f37d5ec61b28a3dffd7cff0a3   \n",
      "3  0x90ab39213f1d947e2a3b8936ac37dca2556a9ece   \n",
      "4  0xa18d7182d5935578958a07fd7e9d062ddd418761   \n",
      "\n",
      "                                            bytecode  block_number  \\\n",
      "0  0x60606040523615610053576000357c01000000000000...         65407   \n",
      "1  0x606060405236156100615760e060020a60003504631c...         66126   \n",
      "2  0x606060405236156100775760e060020a600035046321...         66156   \n",
      "3  0x60606040526000357c01000000000000000000000000...         65327   \n",
      "4  0x60606040526000357c01000000000000000000000000...         66173   \n",
      "\n",
      "                                          block_hash     block_timestamp  \\\n",
      "0  0x0b14125ff9fdb36bef8a566c1a3cf6aa1356a366b2f4... 2015-08-10 19:53:14   \n",
      "1  0x80cb4a1f7d35cad112c1f035a5f31ccf52b450227fa6... 2015-08-10 23:26:20   \n",
      "2  0x4823b4a52e889d9bb4860fba9ee41b69907dbe1d703f... 2015-08-10 23:35:21   \n",
      "3  0x676494743d5df37250b8db3e23f052965f03f5c1af59... 2015-08-10 19:29:48   \n",
      "4  0xc2788ab09a3b0464019df2abc09e5ba9e869c53ef576... 2015-08-10 23:41:31   \n",
      "\n",
      "         date              last_modified  \n",
      "0  2015-08-10 2022-09-11 23:51:39.916963  \n",
      "1  2015-08-10 2022-09-11 23:51:49.252462  \n",
      "2  2015-08-10 2022-09-11 23:51:49.634849  \n",
      "3  2015-08-10 2022-09-11 23:51:38.899058  \n",
      "4  2015-08-10 2022-09-11 23:51:49.854120  \n",
      "Data from s3://aws-public-blockchain/v1.0/eth/contracts/date=2015-08-10/part-00000-3759dbc2-5841-4cd8-b13c-e9776aa78129-c000.snappy.parquet has been successfully loaded into the 'con' table in 'contracts.db'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import sqlite3\n",
    "\n",
    "# Setup access to the S3 bucket\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Specify the directory path in the S3 bucket\n",
    "directory_path = 's3://aws-public-blockchain/v1.0/eth/contracts/date=2015-08-10'\n",
    "files = fs.ls(directory_path)\n",
    "\n",
    "# Filter for .parquet files\n",
    "parquet_files = [file for file in files if file.endswith('.parquet')]\n",
    "\n",
    "# Assuming there's at least one .parquet file, get the path to the first one\n",
    "file_path = 's3://' + parquet_files[0]\n",
    "print(parquet_files[:5])  # Print the first 5 parquet files found\n",
    "\n",
    "# Read the .parquet file into a pandas DataFrame\n",
    "with fs.open(file_path, mode='rb') as f:\n",
    "    df = pd.read_parquet(f, engine='pyarrow')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Use a context manager to handle the SQLite database connection\n",
    "with sqlite3.connect('contracts.db') as conn:\n",
    "    # Create table with primary key, including 'NOT NULL' for clarity\n",
    "    conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS con (\n",
    "        id INTEGER PRIMARY KEY NOT NULL,  \n",
    "        address TEXT,\n",
    "        bytecode TEXT,\n",
    "        block_number INTEGER,\n",
    "        block_hash TEXT,\n",
    "        block_timestamp TEXT,\n",
    "        date TEXT,\n",
    "        last_modified TEXT\n",
    "    );\n",
    "    ''')\n",
    "\n",
    "    # Write the DataFrame to the SQLite table, considering 'if_exists' strategy.\n",
    "    df.to_sql('con', conn, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Data from {file_path} has been successfully loaded into the 'con' table in 'contracts.db'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16e6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-06', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-07', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-09', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-12', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-13', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-14', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-15', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-17', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-18', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-19', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-20', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-21', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-22', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-23', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-24', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-25', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-26', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-27', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-28', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-29', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-30', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-01-31', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-01', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-02', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-03', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-04', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-05', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-06', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-07', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-08', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-09', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-10', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-11', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-12', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-13', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-14', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-15', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-16', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-17', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-18', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-19', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-20', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-21', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-22', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-23', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-24', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-25', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-26', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-27', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-28', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-02-29', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-01', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-02', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-03', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-04', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-05', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-06', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-07', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-08', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-09', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-10', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-11', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-12', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-13', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-14', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-15', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-16', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-17', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-18', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-19', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-20', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-21', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-22', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-23', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-24', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-25', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-26', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-27', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-28', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-29', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-30', 'aws-public-blockchain/v1.0/eth/token_transfers/date=2016-03-31']\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "# Setup access to the S3 bucket\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Base directory path in the S3 bucket\n",
    "base_directory_path = 'aws-public-blockchain/v1.0/eth/token_transfers'\n",
    "\n",
    "# Get the list of date directories\n",
    "date_directories = fs.ls(base_directory_path)\n",
    "\n",
    "# Filter date directories to include only those from 2015\n",
    "date_directories_2015 = [\n",
    "    dir_path for dir_path in date_directories \n",
    "    if 'date=2016-01' in dir_path or 'date=2016-02' in dir_path or 'date=2016-03' in dir_path\n",
    "]\n",
    "# Assuming you want to load .parquet files from the top 20 date directories in 2015\n",
    "top_date_directories_2015 = date_directories_2015\n",
    "\n",
    "# Display the filtered date directories\n",
    "print(top_date_directories_2015)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
